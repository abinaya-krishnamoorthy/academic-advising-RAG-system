# academic-advising-RAG-system
A Retrieval-Augmented Generation AI system built with LangChain and Chroma DB for academic advising queries. Includes data ingestion pipeline, chunking strategies, embeddings, and Streamlit UI.
# Academic Advising RAG System
A Retrieval-Augmented Generation (RAG) application designed to answer academic advising questions using university documents such as degree plans, course descriptions, program requirements, and curriculum worksheets.

This system was built using **Python, LangChain, ChromaDB, and Streamlit**, and demonstrates full development of a real-world RAG pipeline including document ingestion, chunking, vectorization, retrieval, and LLM-based answering.

---

## ğŸš€ Project Overview
The goal of this project is to create an AI assistant that helps students quickly find accurate advising information without manually searching through multiple documents. The system retrieves relevant context from a unified vector database and generates grounded answers.

The project includes:

- A unified data ingestion + preprocessing pipeline  
- Chunking strategies for PDF, Word, Excel, and text files  
- Embedding and vector storage using **ChromaDB**  
- Retrieval pipelines (Similarity + MMR)  
- A Streamlit user interface  
- Timeout handling, no-answer safety, and retrieval-only mode  

---

## ğŸ§  Key Features

### âœ” Multi-format Document Ingestion
Handles:
- PDF files  
- Word (.docx)  
- Excel sheets (.xlsx)  
- Text files  

Data is cleaned, chunked, and stored into a single vector database.

---

### âœ” Custom Chunking Strategies
Implemented efficient chunking for:
- Tabular Excel data  
- Structured PDFs  
- Free-text Word documents  

Each document type uses an optimized approach to maintain context quality.

---

### âœ” Embeddings & Vector Store
- Embedding model: **nomic-embed-text**  
- Vector store: **ChromaDB**  
- Persistent directory with reproducible state  

---

### âœ” Retrieval Pipelines
Supports:
- **Similarity search**  
- **MMR (Max Marginal Relevance)** for diverse results  

With adjustable parameters (via Streamlit UI):
- `top_k`  
- `fetch_k`  
- `lambda` (diversity vs relevance)  

---

### âœ” Streamlit Application
User interface includes:
- Query input box  
- Model selection  
- Retrieval method selector  
- Adjustable retrieval parameters  
- Retrieval-only mode  
- Display of retrieved chunks and context  

---

### âœ” LLM Response Module
- Uses ChatOllama models (e.g., `llama3.2:3b`)  
- Includes system prompts for grounded, context-based answers  
- Timeout guardrails to prevent long-running queries  
- Falls back to â€œinformation unavailable in the contextâ€ when needed  

---

## ğŸ“ Repository Structure

```text
academic-advising-RAG-system/
â”‚
â”œâ”€â”€ app.py                     # Streamlit UI
â”œâ”€â”€ build_index_single.py      # Full data ingestion & indexing pipeline
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ chroma/                    # Vector store directory (gitignored in practice)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Original documents
â”‚   â””â”€â”€ processed/             # Cleaned + chunked data
â”œâ”€â”€ screenshots/               # UI images and sample outputs
â””â”€â”€ README.md                  # Project documentation
